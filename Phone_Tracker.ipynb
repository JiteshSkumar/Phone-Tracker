{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPA1yXCaQpNCFSoBKVx15nr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiteshSkumar/Phone-Tracker/blob/main/Phone_Tracker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install & Imports\n"
      ],
      "metadata": {
        "id": "xdW-8Xr8tyOp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjIthEBqk6TZ",
        "outputId": "c8be1309-96d8-44d1-c6bb-cebc70340c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.192)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.17)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Requirement already satisfied: open_clip_torch in /usr/local/lib/python3.12/dist-packages (3.1.0)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.23.0+cu126)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (2024.11.6)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (6.3.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (4.67.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.34.4)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (0.6.2)\n",
            "Requirement already satisfied: timm>=1.0.17 in /usr/local/lib/python3.12/dist-packages (from open_clip_torch) (1.0.19)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm>=1.0.17->open_clip_torch) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->open_clip_torch) (3.4.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy->open_clip_torch) (0.2.13)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (25.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->open_clip_torch) (1.1.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->open_clip_torch) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->open_clip_torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->open_clip_torch) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->open_clip_torch) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install ultralytics\n",
        "!pip install open_clip_torch\n",
        "\n",
        "import os, cv2, numpy as np, pandas as pd, torch\n",
        "from ultralytics import YOLO\n",
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from collections import deque, Counter\n",
        "from PIL import Image\n",
        "import open_clip\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive"
      ],
      "metadata": {
        "id": "CPXEfK1St5EW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys1sPWN-k-SS",
        "outputId": "1aa9dd25-c643-4d2e-d8f6-64b2ebd37e89"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Folders"
      ],
      "metadata": {
        "id": "qEFCiPJKuBtm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIR  = \"/content/drive/MyDrive/Task Video\"\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/Task Video/op4\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "RUN_STAMP = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "OUTPUT_DIR_RUN = os.path.join(OUTPUT_DIR, f\"run_{RUN_STAMP}\")\n",
        "os.makedirs(OUTPUT_DIR_RUN, exist_ok=True)\n",
        "SUMMARY_CSV = os.path.join(OUTPUT_DIR_RUN, \"summary.csv\")"
      ],
      "metadata": {
        "id": "8_DPZ_JblAtr"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Config & Flags (Accuracy-focused)"
      ],
      "metadata": {
        "id": "4py5ZbpQuIsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HIGH_ACCURACY_MODE   = True\n",
        "DETECTOR_WEIGHTS     = \"yolov8m.pt\" if HIGH_ACCURACY_MODE else \"yolov8s.pt\"\n",
        "IMGSZ_LIST           = [960, 1280] if HIGH_ACCURACY_MODE else [960]\n",
        "USE_HFLIP_TTA        = True if HIGH_ACCURACY_MODE else False\n",
        "CONF_THRESHOLD       = 0.27\n",
        "IOU_NMS              = 0.55\n",
        "MIN_BOX_AREA_PIXELS  = 40 * 40\n",
        "\n",
        "# Activity (multi-cue)\n",
        "BRIGHTNESS_ON_BASE   = 38.0\n",
        "ENTROPY_ON_THRESH    = 3.8\n",
        "DIFF_ON_BASE         = 7.0\n",
        "FLOW_ON_BASE         = 0.85\n",
        "ADAPT_BRIGHT_WEIGHT  = 0.35\n",
        "\n",
        "# Tracking / smoothing\n",
        "IOU_MATCH_THRESHOLD  = 0.35\n",
        "MAX_MISSES           = 20\n",
        "TRACK_HISTORY        = 12\n",
        "SMOOTH_WINDOW        = 15\n",
        "ACTIVE_HYST_ON       = 0.62\n",
        "ACTIVE_HYST_OFF      = 0.45\n",
        "\n",
        "W_BRIGHTNESS = 0.30\n",
        "W_ENTROPY    = 0.25\n",
        "W_DIFF       = 0.25\n",
        "W_FLOW       = 0.20\n",
        "\n",
        "# Logging\n",
        "LOG_NO_PHONE_EVERY   = 60\n",
        "LOG_EVERY_FRAMES     = 100\n",
        "\n",
        "# Shape fallback (for black/off screens)\n",
        "USE_FALLBACK_SHAPE     = True\n",
        "FALLBACK_MIN_AREA_FRAC = 0.003\n",
        "FALLBACK_MAX_AREA_FRAC = 0.25\n",
        "FALLBACK_ASPECT_PORTRAIT = (1.6, 3.1)\n",
        "FALLBACK_ASPECT_LAND     = (0.32, 0.70)\n",
        "FALLBACK_ENTROPY_MAX     = 4.3\n",
        "FALLBACK_MEAN_MAX        = 105.0\n",
        "FALLBACK_MIN_EDGE_DENS   = 0.015\n",
        "\n",
        "# Skip credit-card readers entirely\n",
        "SKIP_POS = True\n",
        "\n",
        "# POS hard filter tuning\n",
        "PHONE_MIN_SIM      = 0.22\n",
        "PHONE_POS_MARGIN   = 0.07\n",
        "POS_MIN_KEYS       = 10\n",
        "POS_KEY_MIN_FRAC   = 0.002\n",
        "POS_KEY_MAX_FRAC   = 0.040\n",
        "POS_KEY_AR_TOL     = 0.35\n",
        "POS_KEY_FILL_MIN   = 0.60\n",
        "POS_KEY_REGION_Y0  = 0.35\n",
        "POS_SLOT_AR_MIN    = 6.0\n",
        "POS_SLOT_MIN_FRAC  = 0.001\n",
        "POS_SLOT_MAX_FRAC  = 0.020\n",
        "POS_SLOT_BORDER_FR = 0.08\n",
        "\n",
        "# CLIP model\n",
        "CLIP_ARCH     = \"ViT-L-14\" if HIGH_ACCURACY_MODE else \"ViT-B-32\"\n",
        "CLIP_CHECKPT  = \"laion2b_s32b_b82k\" if HIGH_ACCURACY_MODE else \"laion2b_s34b_b79k\""
      ],
      "metadata": {
        "id": "G0k8E6bPlDsP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Models: YOLO + CLIP"
      ],
      "metadata": {
        "id": "w_4exGlBuOI8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = YOLO(DETECTOR_WEIGHTS)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, clip_preprocess, clip_tokenizer = open_clip.create_model_and_transforms(\n",
        "    CLIP_ARCH, pretrained=CLIP_CHECKPT, device=DEVICE\n",
        ")\n",
        "\n",
        "PHONE_TEXTS = [\n",
        "    \"a photo of a smartphone\",\n",
        "    \"a mobile phone on a table\",\n",
        "    \"a handheld cell phone\",\n",
        "    \"a touchscreen smartphone\",\n",
        "    \"a cell phone on a table\",\n",
        "    \"a person using a smartphone\",\n",
        "    \"a person using a mobile phone\",\n",
        "    \"a person using a handheld cell phone\",\n",
        "    \"a person using a touchscreen smartphone\",\n",
        "    \"a person using a cell phone on a table\",\n",
        "    \"a hand holding a phone, partially blocked\",\n",
        "    \"a mobile phone partly hidden by a hand\",\n",
        "    \"a phone under a person's hand\",\n",
        "    \"a phone visible behind fingers\",\n",
        "    \"a phone under low lighting\",\n",
        "    \"only part of a smartphone visible\",\n",
        "    \"a hand holding a phone, obscured by motion blur\"\n",
        "\n",
        "]\n",
        "POS_TEXTS = [\n",
        "    \"a point-of-sale terminal\",\n",
        "    \"a credit card machine\",\n",
        "    \"a card reader keypad\",\n",
        "    \"a POS terminal with card slot\",\n",
        "    \"a credit card machine on hand\",\n",
        "    \"a credit card machine on table\",\n",
        "    \"a credit card machine in use\",\n",
        "    \"a person using a credit card machine\",\n",
        "    \"a person using a POS terminal\",\n",
        "    \"a payment terminal at a store counter\",\n",
        "    \"a payment terminal with numeric keypad\",\n",
        "    \"a POS machine connected to a register\",\n",
        "    \"a card swipe machine\",\n",
        "    \"a chip-and-pin reader\",\n",
        "    \"a debit card terminal\",\n",
        "    \"a cash register card reader\",\n",
        "    \"a small POS machine with buttons\",\n",
        "    \"a wired POS terminal\",\n",
        "    \"a point-of-sale keypad device\",\n",
        "    \"a merchant payment terminal\"\n",
        "]\n",
        "with torch.no_grad():\n",
        "    phone_tokens = open_clip.tokenize(PHONE_TEXTS).to(DEVICE)\n",
        "    pos_tokens   = open_clip.tokenize(POS_TEXTS).to(DEVICE)\n",
        "    phone_text_feat = clip_model.encode_text(phone_tokens)\n",
        "    pos_text_feat   = clip_model.encode_text(pos_tokens)\n",
        "    phone_text_feat = (phone_text_feat / phone_text_feat.norm(dim=-1, keepdim=True)).mean(dim=0, keepdim=True)\n",
        "    pos_text_feat   = (pos_text_feat   / pos_text_feat.norm(dim=-1, keepdim=True)).mean(dim=0, keepdim=True)"
      ],
      "metadata": {
        "id": "RKcqJOp0s3Ne"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helpers (WBF, detection, features)"
      ],
      "metadata": {
        "id": "qe-l0zaLuVIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def to_xyxy_norm(box, W, H):\n",
        "    x1,y1,x2,y2 = box\n",
        "    return [x1/W, y1/H, x2/W, y2/H]\n",
        "\n",
        "def to_xyxy_abs(box, W, H):\n",
        "    x1,y1,x2,y2 = box\n",
        "    return [int(x1*W), int(y1*H), int(x2*W), int(y2*H)]\n",
        "\n",
        "def iou_xyxy(a, b):\n",
        "    x1 = max(a[0], b[0]); y1 = max(a[1], b[1])\n",
        "    x2 = min(a[2], b[2]); y2 = min(a[3], b[3])\n",
        "    inter = max(0, x2-x1) * max(0, y2-y1)\n",
        "    a1 = (a[2]-a[0])*(a[3]-a[1]); a2 = (b[2]-b[0])*(b[3]-b[1])\n",
        "    return inter / (a1 + a2 - inter + 1e-6)\n",
        "\n",
        "def weighted_boxes_fusion(boxes, scores, iou_thr=0.55):\n",
        "    if not boxes: return [], []\n",
        "    idxs = np.argsort(scores)[::-1]\n",
        "    boxes = [boxes[i] for i in idxs]\n",
        "    scores = [scores[i] for i in idxs]\n",
        "    fused_boxes, fused_scores, used = [], [], [False]*len(boxes)\n",
        "    for i in range(len(boxes)):\n",
        "        if used[i]: continue\n",
        "        cluster = [i]; used[i] = True\n",
        "        for j in range(i+1, len(boxes)):\n",
        "            if used[j]: continue\n",
        "            if iou_xyxy(boxes[i], boxes[j]) >= iou_thr:\n",
        "                used[j] = True; cluster.append(j)\n",
        "        if len(cluster) == 1:\n",
        "            fused_boxes.append(boxes[i]); fused_scores.append(scores[i])\n",
        "        else:\n",
        "            wsum = sum(scores[k] for k in cluster) + 1e-9\n",
        "            bx = [0,0,0,0]; sc = 0.0\n",
        "            for k in cluster:\n",
        "                s = scores[k]; sc += s\n",
        "                bx[0] += boxes[k][0]*s; bx[1] += boxes[k][1]*s\n",
        "                bx[2] += boxes[k][2]*s; bx[3] += boxes[k][3]*s\n",
        "            fused_boxes.append([b/wsum for b in bx]); fused_scores.append(sc/len(cluster))\n",
        "    return fused_boxes, fused_scores\n",
        "\n",
        "def yolo_tta_predict(frame_bgr):\n",
        "    H, W = frame_bgr.shape[:2]\n",
        "    boxes_norm, scores = [], []\n",
        "    for s in IMGSZ_LIST:\n",
        "        r = model.predict(frame_bgr, imgsz=s, conf=CONF_THRESHOLD, iou=IOU_NMS,\n",
        "                          agnostic_nms=True, verbose=False)[0]\n",
        "        names = r.names\n",
        "        for b in r.boxes:\n",
        "            cls_id = int(b.cls.item()); conf = float(b.conf.item())\n",
        "            name = names.get(cls_id, str(cls_id)) if isinstance(names, dict) else names[cls_id]\n",
        "            if name and \"phone\" in name.lower():\n",
        "                x1,y1,x2,y2 = map(int, b.xyxy[0].tolist())\n",
        "                if (x2-x1)*(y2-y1) >= MIN_BOX_AREA_PIXELS:\n",
        "                    boxes_norm.append(to_xyxy_norm([x1,y1,x2,y2], W, H))\n",
        "                    scores.append(conf)\n",
        "        if USE_HFLIP_TTA:\n",
        "            f = cv2.flip(frame_bgr, 1)\n",
        "            r2 = model.predict(f, imgsz=s, conf=CONF_THRESHOLD, iou=IOU_NMS,\n",
        "                               agnostic_nms=True, verbose=False)[0]\n",
        "            for b in r2.boxes:\n",
        "                cls_id = int(b.cls.item()); conf = float(b.conf.item())\n",
        "                name = r2.names.get(cls_id, str(cls_id)) if isinstance(r2.names, dict) else r2.names[cls_id]\n",
        "                if name and \"phone\" in name.lower():\n",
        "                    x1,y1,x2,y2 = map(int, b.xyxy[0].tolist())\n",
        "                    x1f, x2f = W - x2, W - x1\n",
        "                    if (x2f-x1f)*(y2-y1) >= MIN_BOX_AREA_PIXELS:\n",
        "                        boxes_norm.append(to_xyxy_norm([x1f,y1,x2f,y2], W, H))\n",
        "                        scores.append(conf)\n",
        "    fused_boxes_norm, fused_scores = weighted_boxes_fusion(boxes_norm, scores, iou_thr=0.55)\n",
        "    fused_boxes_abs = [to_xyxy_abs(b, W, H) for b in fused_boxes_norm]\n",
        "    return fused_boxes_abs, fused_scores\n",
        "\n",
        "def safe_crop(frame, x1, y1, x2, y2):\n",
        "    h, w = frame.shape[:2]\n",
        "    x1, y1 = max(0, x1), max(0, y1); x2, y2 = min(w, x2), min(h, y2)\n",
        "    if x2 <= x1 or y2 <= y1: return None\n",
        "    return frame[y1:y2, x1:x2]\n",
        "\n",
        "def shannon_entropy(gray_roi):\n",
        "    if gray_roi is None or gray_roi.size == 0: return 0.0\n",
        "    hist = cv2.calcHist([gray_roi], [0], None, [256], [0,256]).ravel()\n",
        "    p = hist / (np.sum(hist) + 1e-9); p = p[p>0]\n",
        "    return float(-np.sum(p * np.log2(p)))\n",
        "\n",
        "def mean_abs_diff(prev_gray, curr_gray, bbox):\n",
        "    if prev_gray is None or curr_gray is None or bbox is None: return 0.0\n",
        "    x1,y1,x2,y2 = bbox\n",
        "    h, w = curr_gray.shape[:2]\n",
        "    x1, y1 = max(0, x1), max(0, y1); x2, y2 = min(w, x2), min(h, y2)\n",
        "    if x2<=x1 or y2<=y1: return 0.0\n",
        "    prev_roi = prev_gray[y1:y2, x1:x2]; curr_roi = curr_gray[y1:y2, x1:x2]\n",
        "    if prev_roi.size==0 or curr_roi.size==0 or prev_roi.shape!=curr_roi.shape: return 0.0\n",
        "    diff = cv2.absdiff(prev_roi, curr_roi)\n",
        "    return float(np.mean(diff))\n",
        "\n",
        "def dense_flow_mag(prev_gray, curr_gray, bbox):\n",
        "    if prev_gray is None or curr_gray is None or bbox is None: return 0.0\n",
        "    x1,y1,x2,y2 = bbox\n",
        "    h, w = curr_gray.shape[:2]\n",
        "    x1, y1 = max(0, x1), max(0, y1); x2, y2 = min(w, x2), min(h, y2)\n",
        "    if x2<=x1 or y2<=y1: return 0.0\n",
        "    prev_roi = prev_gray[y1:y2, x1:x2]; curr_roi = curr_gray[y1:y2, x1:x2]\n",
        "    if prev_roi.size==0 or curr_roi.size==0: return 0.0\n",
        "    flow = cv2.calcOpticalFlowFarneback(prev_roi, curr_roi, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "    mag, _ = cv2.cartToPolar(flow[...,0], flow[...,1])\n",
        "    return float(np.mean(mag))\n",
        "\n",
        "def classify_phone_vs_pos(roi_bgr):\n",
        "    if roi_bgr is None or roi_bgr.size == 0:\n",
        "        return \"unknown\", 0.0, 0.0\n",
        "    rgb = cv2.cvtColor(roi_bgr, cv2.COLOR_BGR2RGB)\n",
        "    pil = Image.fromarray(rgb)\n",
        "    with torch.no_grad():\n",
        "        img = clip_preprocess(pil).unsqueeze(0).to(DEVICE)\n",
        "        img_feat = clip_model.encode_image(img)\n",
        "        img_feat = img_feat / img_feat.norm(dim=-1, keepdim=True)\n",
        "        phone_sim = (img_feat @ phone_text_feat.T).item()\n",
        "        pos_sim   = (img_feat @ pos_text_feat.T).item()\n",
        "    return (\"phone\" if phone_sim >= pos_sim else \"pos\"), float(phone_sim), float(pos_sim)\n",
        "\n",
        "def draw_box(frame, bbox, status, track_id=None, conf=None, obj_type=\"phone\"):\n",
        "    x1,y1,x2,y2 = bbox\n",
        "    color = (0,255,0) if (obj_type==\"phone\" and status==\"Active\") else (0,0,255) if obj_type==\"phone\" else (255,0,0)\n",
        "    label = f\"ID {track_id} | {status if obj_type=='phone' else 'POS'}\"\n",
        "    if conf is not None: label += f\" ({conf:.2f})\"\n",
        "    if status == \"Active\" and obj_type=='phone':\n",
        "      cv2.rectangle(frame, (x1,y1), (x2,y2), color, 2)\n",
        "      (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
        "      cv2.rectangle(frame, (x1, y1 - th - 8), (x1 + tw + 4, y1), color, -1)\n",
        "      cv2.putText(frame, label, (x1 + 2, y1 - 4),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2, cv2.LINE_AA)\n",
        "\n",
        "# Improved shape-based fallback\n",
        "def shape_phone_candidates(gray, frame_area):\n",
        "    if not USE_FALLBACK_SHAPE: return []\n",
        "    min_area = max(int(FALLBACK_MIN_AREA_FRAC*frame_area), MIN_BOX_AREA_PIXELS)\n",
        "    max_area = int(FALLBACK_MAX_AREA_FRAC*frame_area)\n",
        "\n",
        "    edges = cv2.Canny(gray, 60, 160)\n",
        "    edges = cv2.dilate(edges, None, iterations=1)\n",
        "    cnts, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    cands = []\n",
        "    for c in cnts:\n",
        "        area = cv2.contourArea(c)\n",
        "        if area < min_area or area > max_area: continue\n",
        "        x,y,w,h = cv2.boundingRect(c)\n",
        "        if w<=0 or h<=0: continue\n",
        "\n",
        "        rect_area = w*h\n",
        "        edge_count = np.count_nonzero(edges[y:y+h, x:x+w])\n",
        "        edge_density = edge_count / (rect_area + 1e-9)\n",
        "        if edge_density < FALLBACK_MIN_EDGE_DENS:\n",
        "            continue\n",
        "\n",
        "        ar = max(w,h)/(min(w,h)+1e-6)\n",
        "        portrait_ok  = (FALLBACK_ASPECT_PORTRAIT[0] <= ar <= FALLBACK_ASPECT_PORTRAIT[1])\n",
        "        landscape_ok = (FALLBACK_ASPECT_LAND[0]     <= 1.0/ar <= FALLBACK_ASPECT_LAND[1])\n",
        "        if not (portrait_ok or landscape_ok): continue\n",
        "\n",
        "        peri = cv2.arcLength(c, True)\n",
        "        approx = cv2.approxPolyDP(c, 0.02*peri, True)\n",
        "        if len(approx) < 4 or not cv2.isContourConvex(approx): continue\n",
        "\n",
        "        roi = gray[y:y+h, x:x+w]\n",
        "        m = float(np.mean(roi)); ent = shannon_entropy(roi)\n",
        "        if m <= FALLBACK_MEAN_MAX and ent <= FALLBACK_ENTROPY_MAX:\n",
        "            ar_target = 2.0 if h>=w else 0.5\n",
        "            ar_score = 1.0/(1.0 + abs(ar - max(ar_target,1.0)))\n",
        "            darkness = np.clip((FALLBACK_MEAN_MAX - m)/max(FALLBACK_MEAN_MAX,1e-6), 0, 1)\n",
        "            entropy_gain = np.clip((FALLBACK_ENTROPY_MAX - ent)/max(FALLBACK_ENTROPY_MAX,1e-6), 0, 1)\n",
        "            score = 0.45*darkness + 0.25*entropy_gain + 0.30*ar_score\n",
        "            cands.append((x, y, x+w, y+h, float(score)))\n",
        "\n",
        "    if not cands: return []\n",
        "    boxes = np.array([c[:4] for c in cands], dtype=np.float32)\n",
        "    scores = np.array([c[4] for c in cands], dtype=np.float32)\n",
        "    keep = []\n",
        "    idxs = scores.argsort()[::-1]\n",
        "    def iou(a,b):\n",
        "        x1=max(a[0],b[0]); y1=max(a[1],b[1]); x2=min(a[2],b[2]); y2=min(a[3],b[3])\n",
        "        inter=max(0,x2-x1)*max(0,y2-y1); A=(a[2]-a[0])*(a[3]-a[1]); B=(b[2]-b[0])*(b[3]-b[1])\n",
        "        return inter/(A+B-inter+1e-6)\n",
        "    while len(idxs):\n",
        "        i = idxs[0]; keep.append(i)\n",
        "        suppress = [0]\n",
        "        for pos in range(1, len(idxs)):\n",
        "            j = idxs[pos]\n",
        "            if iou(boxes[i], boxes[j]) > 0.5:\n",
        "                suppress.append(pos)\n",
        "        idxs = np.delete(idxs, suppress)\n",
        "    return [cands[i] for i in keep]\n",
        "\n",
        "# POS heuristic (keypad/slot)\n",
        "def likely_pos_heuristic(gray_roi):\n",
        "    \"\"\"\n",
        "    Heuristic POS detector:\n",
        "    - Many near-square 'keys' in bottom region (counts small rectangles)\n",
        "    - Or a long, thin 'slot' near an ROI edge\n",
        "    Returns True if likely a POS/card reader.\n",
        "    \"\"\"\n",
        "    if gray_roi is None or gray_roi.size == 0:\n",
        "        return False\n",
        "\n",
        "    h, w = gray_roi.shape[:2]\n",
        "    roi_area = float(h * w)\n",
        "\n",
        "    # Bottom keypad region\n",
        "    y0 = int(POS_KEY_REGION_Y0 * h)\n",
        "    bot = gray_roi[y0:h, :]\n",
        "\n",
        "    thr = cv2.adaptiveThreshold(bot, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
        "                                cv2.THRESH_BINARY_INV, 31, 7)\n",
        "    edges = cv2.Canny(bot, 60, 160)\n",
        "    fused = cv2.bitwise_or(thr, edges)\n",
        "    fused = cv2.morphologyEx(fused, cv2.MORPH_CLOSE, np.ones((3,3), np.uint8), iterations=1)\n",
        "\n",
        "    cnts, _ = cv2.findContours(fused, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    key_like = 0\n",
        "    for c in cnts:\n",
        "        area = cv2.contourArea(c)\n",
        "        if area <= 1:\n",
        "            continue\n",
        "        frac = area / roi_area\n",
        "        if not (POS_KEY_MIN_FRAC <= frac <= POS_KEY_MAX_FRAC):\n",
        "            continue\n",
        "\n",
        "        x, y, bw, bh = cv2.boundingRect(c)\n",
        "        if bw == 0 or bh == 0:\n",
        "            continue\n",
        "        ar = bw / float(bh)\n",
        "        if abs(1.0 - ar) > POS_KEY_AR_TOL:\n",
        "            continue\n",
        "\n",
        "        rect_area = bw * bh\n",
        "        fill = area / (rect_area + 1e-6)\n",
        "        if fill < POS_KEY_FILL_MIN:\n",
        "            continue\n",
        "\n",
        "        key_like += 1\n",
        "        if key_like >= POS_MIN_KEYS:\n",
        "            return True\n",
        "\n",
        "    # Card slot near border\n",
        "    edges_full = cv2.Canny(gray_roi, 60, 160)\n",
        "    edges_full = cv2.dilate(edges_full, None, iterations=1)\n",
        "    cnts2, _ = cv2.findContours(edges_full, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    border_margin = POS_SLOT_BORDER_FR * min(h, w)\n",
        "\n",
        "    for c in cnts2:\n",
        "        area = cv2.contourArea(c)\n",
        "        if area <= 1:\n",
        "            continue\n",
        "        frac = area / roi_area\n",
        "        if not (POS_SLOT_MIN_FRAC <= frac <= POS_SLOT_MAX_FRAC):\n",
        "            continue\n",
        "        x, y, bw, bh = cv2.boundingRect(c)\n",
        "        if bw == 0 or bh == 0:\n",
        "            continue\n",
        "        ar_long = max(bw, bh) / float(min(bw, bh) + 1e-6)\n",
        "        if ar_long < POS_SLOT_AR_MIN:\n",
        "            continue\n",
        "        near_border = (x <= border_margin or y <= border_margin or\n",
        "                       (w - (x + bw)) <= border_margin or\n",
        "                       (h - (y + bh)) <= border_margin)\n",
        "        if near_border:\n",
        "            return True\n",
        "\n",
        "    return False"
      ],
      "metadata": {
        "id": "6VO1uPKptLDr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activity scoring + hysteresis"
      ],
      "metadata": {
        "id": "FAWcZ_dIul7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def activity_score(bright, ent, diff_m, flow_m, b_on, e_on, d_on, f_on):\n",
        "    sb = np.clip((bright/(b_on+1e-6)), 0, 1.5)/1.5\n",
        "    se = np.clip((ent/(e_on+1e-6)),    0, 1.5)/1.5\n",
        "    sd = np.clip((diff_m/(d_on+1e-6)), 0, 1.5)/1.5\n",
        "    sf = np.clip((flow_m/(f_on+1e-6)), 0, 1.5)/1.5\n",
        "    return float(W_BRIGHTNESS*sb + W_ENTROPY*se + W_DIFF*sd + W_FLOW*sf)\n",
        "\n",
        "def hysteresis_label(prev_label, smoothed_score, on_thr=ACTIVE_HYST_ON, off_thr=ACTIVE_HYST_OFF):\n",
        "    if prev_label == \"Active\":\n",
        "        return \"Active\" if smoothed_score >= off_thr else \"Idle\"\n",
        "    else:\n",
        "        return \"Active\" if smoothed_score >= on_thr else \"Idle\""
      ],
      "metadata": {
        "id": "JCqC6LSKtXZD"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Simple IOU Tracker"
      ],
      "metadata": {
        "id": "7Yp841bQuqp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Track:\n",
        "    _next_id = 1\n",
        "    def __init__(self, bbox, conf, obj_type=\"phone\"):\n",
        "        self.id = Track._next_id; Track._next_id += 1\n",
        "        self.bbox = bbox; self.conf = conf; self.misses = 0\n",
        "        self.history = deque(maxlen=TRACK_HISTORY)\n",
        "        self.total_frames = 0; self.active_frames = 0; self.idle_frames = 0\n",
        "        self.score_window = deque(maxlen=SMOOTH_WINDOW)\n",
        "        self.last_label = \"Idle\"\n",
        "        self.last_features = dict()\n",
        "        self.type_window = deque(maxlen=SMOOTH_WINDOW)\n",
        "        self.obj_type = obj_type\n",
        "\n",
        "    def stable_type(self):\n",
        "        if not self.type_window: return self.obj_type\n",
        "        cnt = Counter(self.type_window)\n",
        "        return \"phone\" if cnt[\"phone\"] >= cnt[\"pos\"] else \"pos\"\n",
        "\n",
        "    def update(self, bbox, conf, score, label, features, obj_type_now=None):\n",
        "        self.bbox = bbox; self.conf = conf; self.misses = 0\n",
        "        self.total_frames += 1\n",
        "        cx = int((bbox[0]+bbox[2])/2); cy = int((bbox[1]+bbox[3])/2)\n",
        "        self.history.append((cx, cy))\n",
        "        if score is not None: self.score_window.append(score)\n",
        "        if obj_type_now is not None:\n",
        "            self.type_window.append(obj_type_now)\n",
        "            self.obj_type = self.stable_type()\n",
        "        self.last_label = label; self.last_features = features\n",
        "        if self.obj_type == \"phone\":\n",
        "            if label == \"Active\": self.active_frames += 1\n",
        "            else: self.idle_frames += 1\n",
        "\n",
        "def match_tracks(tracks, detections, confs, iou_th=IOU_MATCH_THRESHOLD):\n",
        "    if not tracks or not detections:\n",
        "        return set(range(len(detections))), tracks[:]\n",
        "    iou_mat = np.zeros((len(tracks), len(detections)), dtype=np.float32)\n",
        "    for ti, t in enumerate(tracks):\n",
        "        for di, d in enumerate(detections):\n",
        "            iou_mat[ti, di] = iou_xyxy(t.bbox, d)\n",
        "    flat = [(i, j, iou_mat[i, j]) for i in range(len(tracks)) for j in range(len(detections))]\n",
        "    used_tracks, used_dets = set(), set()\n",
        "    for ti, di, v in sorted(flat, key=lambda x: x[2], reverse=True):\n",
        "        if v < iou_th: break\n",
        "        if ti in used_tracks or di in used_dets: continue\n",
        "        tracks[ti].bbox = detections[di]; tracks[ti].conf = confs[di]; tracks[ti].misses = 0\n",
        "        used_tracks.add(ti); used_dets.add(di)\n",
        "    unmatched_det_idx = set(range(len(detections))) - used_dets\n",
        "    unmatched_tracks  = [tracks[ti] for ti in range(len(tracks)) if ti not in used_tracks]\n",
        "    return unmatched_det_idx, unmatched_tracks"
      ],
      "metadata": {
        "id": "Xd0H9ccMtbvh"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Video Processor"
      ],
      "metadata": {
        "id": "DHv_l595uuxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(input_path, output_path, log_every=LOG_EVERY_FRAMES):\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "    if not cap.isOpened():\n",
        "        print(f\"[WARN] Could not open: {input_path}\")\n",
        "        return None\n",
        "\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "    w   = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)); h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out    = cv2.VideoWriter(output_path, fourcc, fps, (w, h))\n",
        "\n",
        "    prev_gray = None; frame_idx = 0\n",
        "    total_frames = 0; phone_frames = 0\n",
        "    any_phone_detected = False\n",
        "    tracks = []\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "        total_frames += 1; frame_idx += 1\n",
        "\n",
        "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "        global_bright = float(np.mean(gray))\n",
        "        BRIGHTNESS_ON = max(20.0, BRIGHTNESS_ON_BASE + ADAPT_BRIGHT_WEIGHT*(global_bright - 50))\n",
        "\n",
        "        # Detector with TTA + WBF\n",
        "        yolo_boxes, yolo_scores = yolo_tta_predict(frame)\n",
        "\n",
        "        # Fallback: shape candidates if YOLO empty\n",
        "        shape_added = False\n",
        "        if not yolo_boxes and USE_FALLBACK_SHAPE:\n",
        "            frame_area = w*h\n",
        "            cands = shape_phone_candidates(gray, frame_area)\n",
        "            if cands:\n",
        "                shape_added = True\n",
        "                for (x1,y1,x2,y2,s) in cands:\n",
        "                    yolo_boxes.append([int(x1), int(y1), int(x2), int(y2)])\n",
        "                    yolo_scores.append(float(max(0.2, s)))\n",
        "\n",
        "        det_bboxes, det_confs = [], []\n",
        "        det_scores, det_feats, det_type = [], [], []\n",
        "        phones_this_frame = 0\n",
        "\n",
        "        if yolo_boxes:\n",
        "            for (x1,y1,x2,y2), conf in zip(yolo_boxes, yolo_scores):\n",
        "                if (x2-x1)*(y2-y1) < MIN_BOX_AREA_PIXELS: continue\n",
        "                roi = safe_crop(frame, x1, y1, x2, y2)\n",
        "                if roi is None: continue\n",
        "\n",
        "                # classify phone vs POS via CLIP\n",
        "                obj_type, sim_phone, sim_pos = classify_phone_vs_pos(roi)\n",
        "                gray_roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "                # extra POS filters (hard block)\n",
        "                is_pos_heur = likely_pos_heuristic(gray_roi)\n",
        "                phone_wins_margin = (sim_phone - sim_pos) >= PHONE_POS_MARGIN\n",
        "                phone_above_min   = (sim_phone >= PHONE_MIN_SIM)\n",
        "                treat_as_pos = (obj_type == \"pos\") or is_pos_heur or (not phone_wins_margin) or (not phone_above_min)\n",
        "\n",
        "                if SKIP_POS and treat_as_pos:\n",
        "                    continue\n",
        "\n",
        "                # Treat as PHONE\n",
        "                bright = float(np.mean(gray_roi))\n",
        "                ent    = shannon_entropy(gray_roi)\n",
        "                diff_m = mean_abs_diff(prev_gray, gray, (x1, y1, x2, y2))\n",
        "                flow_m = dense_flow_mag(prev_gray, gray, (x1, y1, x2, y2))\n",
        "\n",
        "                score_now = activity_score(\n",
        "                    bright, ent, diff_m, flow_m,\n",
        "                    BRIGHTNESS_ON, ENTROPY_ON_THRESH, DIFF_ON_BASE, FLOW_ON_BASE\n",
        "                )\n",
        "                feats = {\n",
        "                    \"bright\": bright, \"entropy\": ent,\n",
        "                    \"diff\": diff_m, \"flow\": flow_m,\n",
        "                    \"bright_thr\": BRIGHTNESS_ON,\n",
        "                    \"src\": \"shape\" if shape_added else \"yolo\",\n",
        "                    \"phone_sim\": sim_phone, \"pos_sim\": sim_pos,\n",
        "                    \"pos_heur\": bool(is_pos_heur),\n",
        "                    \"margin_ok\": bool(phone_wins_margin),\n",
        "                    \"minsim_ok\": bool(phone_above_min),\n",
        "                }\n",
        "\n",
        "                det_bboxes.append([x1,y1,x2,y2])\n",
        "                det_confs.append(conf)\n",
        "                det_scores.append(score_now)\n",
        "                det_feats.append(feats)\n",
        "                det_type.append(\"phone\")\n",
        "                phones_this_frame += 1\n",
        "\n",
        "            if phones_this_frame > 0:\n",
        "                any_phone_detected = True\n",
        "                phone_frames += 1\n",
        "            '''elif frame_idx % LOG_NO_PHONE_EVERY == 0:\n",
        "                print(f\"[{os.path.basename(input_path)}] No mobile found (frame {frame_idx}).\")\n",
        "        else:\n",
        "            if frame_idx % LOG_NO_PHONE_EVERY == 0:\n",
        "                print(f\"[{os.path.basename(input_path)}] No mobile found (frame {frame_idx}).\")'''\n",
        "\n",
        "        # Track association\n",
        "        unmatched_det_idx, unmatched_tracks = match_tracks(tracks, det_bboxes, det_confs)\n",
        "\n",
        "        # Age unmatched tracks\n",
        "        kept = []\n",
        "        for t in tracks:\n",
        "            if t in unmatched_tracks:\n",
        "                t.misses += 1\n",
        "                if t.misses <= MAX_MISSES: kept.append(t)\n",
        "            else:\n",
        "                kept.append(t)\n",
        "        tracks = kept\n",
        "\n",
        "        # New tracks\n",
        "        for di in unmatched_det_idx:\n",
        "            t = Track(det_bboxes[di], det_confs[di], obj_type=det_type[di]); tracks.append(t)\n",
        "\n",
        "        # Smoothing + hysteresis\n",
        "        det_map = {tuple(det_bboxes[i]): (det_scores[i], det_feats[i], det_type[i]) for i in range(len(det_bboxes))}\n",
        "        for t in tracks:\n",
        "            key = tuple(t.bbox)\n",
        "            if key in det_map:\n",
        "                score_now, feats, obj_type_now = det_map[key]\n",
        "                if t.obj_type == \"phone\":\n",
        "                    scores_for_win = (list(t.score_window) + [score_now]) if score_now is not None else list(t.score_window)\n",
        "                    smooth_score = float(np.mean(scores_for_win)) if len(scores_for_win) else 0.0\n",
        "                    new_label = hysteresis_label(t.last_label, smooth_score, ACTIVE_HYST_ON, ACTIVE_HYST_OFF)\n",
        "                else:\n",
        "                    smooth_score = None\n",
        "                    new_label = \"POS\"\n",
        "                t.update(t.bbox, t.conf, smooth_score, new_label, feats, obj_type_now=obj_type_now)\n",
        "\n",
        "        # Draw\n",
        "        for t in tracks:\n",
        "            if t.misses <= MAX_MISSES and t.obj_type == \"phone\":\n",
        "                draw_box(frame, t.bbox, t.last_label, track_id=t.id, conf=t.conf, obj_type=t.obj_type)\n",
        "                for k in range(1, len(t.history)):\n",
        "                    cv2.line(frame, t.history[k-1], t.history[k], (255,255,255), 2)\n",
        "\n",
        "        out.write(frame)\n",
        "        prev_gray = gray\n",
        "\n",
        "        if log_every and frame_idx % log_every == 0:\n",
        "            print(f\"[{os.path.basename(input_path)}] Processed {frame_idx} frames...\")\n",
        "\n",
        "    cap.release(); out.release()\n",
        "\n",
        "    if not any_phone_detected:\n",
        "        print(f\"[{os.path.basename(input_path)}] No mobile found in entire video.\")\n",
        "\n",
        "    # CSV rows\n",
        "    track_rows = []\n",
        "    for t in tracks:\n",
        "        if t.total_frames > 0 and t.obj_type == \"phone\":\n",
        "            track_rows.append({\n",
        "                \"video\": os.path.basename(input_path),\n",
        "                \"track_id\": t.id,\n",
        "                \"type\": t.obj_type,\n",
        "                \"total_frames_for_track\": t.total_frames,\n",
        "                \"active_frames_for_track\": t.active_frames,\n",
        "                \"idle_frames_for_track\": t.idle_frames,\n",
        "                \"active_ratio_for_track\": (t.active_frames / max(1, t.total_frames))\n",
        "            })\n",
        "\n",
        "    summary_row = {\n",
        "        \"video\": os.path.basename(input_path),\n",
        "        \"total_frames\": total_frames,\n",
        "        \"phone_frames_with_any_detection\": phone_frames,\n",
        "        \"num_tracks\": len(track_rows)\n",
        "    }\n",
        "    return summary_row, track_rows"
      ],
      "metadata": {
        "id": "huFxyIuMtcYi"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch over all videos (save outputs WITHOUT input file names)"
      ],
      "metadata": {
        "id": "Ol_dmp6Yu1Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "video_exts = {\".mp4\", \".mov\", \".avi\", \".mkv\", \".MP4\", \".MOV\", \".AVI\", \".MKV\"}\n",
        "input_files = [str(p) for p in Path(INPUT_DIR).glob(\"*\") if p.suffix in video_exts]\n",
        "input_files.sort()\n",
        "\n",
        "if not input_files:\n",
        "    print(f\"No videos found in: {INPUT_DIR}. Add files with: {sorted(video_exts)}\")\n",
        "\n",
        "summary_rows, all_track_rows = [], []\n",
        "for i, vid in enumerate(input_files, start=1):\n",
        "    out_name = f\"processed_{i:04d}.mp4\"            # <-- no original filename used\n",
        "    out_path = os.path.join(OUTPUT_DIR_RUN, out_name)\n",
        "    print(f\"\\n=== Processing video #{i}: {os.path.basename(vid)} -> {out_name} ===\")\n",
        "    res = process_video(vid, out_path)\n",
        "    if res is not None:\n",
        "        srow, trows = res\n",
        "        summary_rows.append(srow); all_track_rows.extend(trows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jq-pnsx5tfVs",
        "outputId": "38a2fabd-cfa3-48f4-8e5c-bfdd1904c28a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Processing video #1: 20250715_142638_e37e7821.mp4 -> processed_0001.mp4 ===\n",
            "\n",
            "=== Processing video #2: 20250715_144536_9170ec05.mp4 -> processed_0002.mp4 ===\n",
            "\n",
            "=== Processing video #3: 20250715_160539_6c132ed0.mp4 -> processed_0003.mp4 ===\n",
            "[20250715_160539_6c132ed0.mp4] No mobile found in entire video.\n",
            "\n",
            "=== Processing video #4: 20250715_173316_8cb090ab.mp4 -> processed_0004.mp4 ===\n",
            "\n",
            "=== Processing video #5: 20250718_145802_46039155.mp4 -> processed_0005.mp4 ===\n",
            "[20250718_145802_46039155.mp4] Processed 100 frames...\n",
            "\n",
            "=== Processing video #6: 20250718_150650_075a44fc.mp4 -> processed_0006.mp4 ===\n",
            "[20250718_150650_075a44fc.mp4] Processed 100 frames...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save CSVs"
      ],
      "metadata": {
        "id": "-xOFxwvnu69i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if summary_rows:\n",
        "    df_sum = pd.DataFrame(summary_rows)\n",
        "    df_tracks = pd.DataFrame(all_track_rows) if all_track_rows else pd.DataFrame(\n",
        "        columns=[\"video\",\"track_id\",\"type\",\"total_frames_for_track\",\"active_frames_for_track\",\"idle_frames_for_track\",\"active_ratio_for_track\"]\n",
        "    )\n",
        "    sum_path    = SUMMARY_CSV\n",
        "    tracks_path = os.path.join(OUTPUT_DIR_RUN, \"tracks.csv\")\n",
        "    df_sum.to_csv(sum_path, index=False); df_tracks.to_csv(tracks_path, index=False)\n",
        "\n",
        "    print(\"\\nPer-video summary:\"); display(df_sum.head())\n",
        "    print(\"\\nPer-track summary (first 20 rows):\"); display(df_tracks.head(20))\n",
        "    print(f\"\\nSaved:\\n- {sum_path}\\n- {tracks_path}\\n- Annotated videos in: {OUTPUT_DIR_RUN}\")\n",
        "else:\n",
        "    print(\"No results to summarize.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "elhtI74Ithgq",
        "outputId": "c792b97e-443b-4fc0-fa7e-c35f09d679d0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-video summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                          video  total_frames  \\\n",
              "0  20250715_142638_e37e7821.mp4            80   \n",
              "1  20250715_144536_9170ec05.mp4            70   \n",
              "2  20250715_160539_6c132ed0.mp4            80   \n",
              "3  20250715_173316_8cb090ab.mp4            80   \n",
              "4  20250718_145802_46039155.mp4           180   \n",
              "\n",
              "   phone_frames_with_any_detection  num_tracks  \n",
              "0                               13           1  \n",
              "1                               31           1  \n",
              "2                                0           0  \n",
              "3                               27           1  \n",
              "4                              179           2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c1ed271b-ccf7-4210-b699-5bc608f591d9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video</th>\n",
              "      <th>total_frames</th>\n",
              "      <th>phone_frames_with_any_detection</th>\n",
              "      <th>num_tracks</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20250715_142638_e37e7821.mp4</td>\n",
              "      <td>80</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20250715_144536_9170ec05.mp4</td>\n",
              "      <td>70</td>\n",
              "      <td>31</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20250715_160539_6c132ed0.mp4</td>\n",
              "      <td>80</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20250715_173316_8cb090ab.mp4</td>\n",
              "      <td>80</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20250718_145802_46039155.mp4</td>\n",
              "      <td>180</td>\n",
              "      <td>179</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c1ed271b-ccf7-4210-b699-5bc608f591d9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c1ed271b-ccf7-4210-b699-5bc608f591d9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c1ed271b-ccf7-4210-b699-5bc608f591d9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-03663373-0861-4e66-a782-a45f41820c36\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-03663373-0861-4e66-a782-a45f41820c36')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-03663373-0861-4e66-a782-a45f41820c36 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"No results to summarize\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"video\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"20250715_144536_9170ec05.mp4\",\n          \"20250718_145802_46039155.mp4\",\n          \"20250715_160539_6c132ed0.mp4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_frames\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 46,\n        \"min\": 70,\n        \"max\": 180,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          80,\n          70,\n          180\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"phone_frames_with_any_detection\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 73,\n        \"min\": 0,\n        \"max\": 179,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          31,\n          179,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"num_tracks\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          0,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Per-track summary (first 20 rows):\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                          video  track_id   type  total_frames_for_track  \\\n",
              "0  20250715_142638_e37e7821.mp4         3  phone                       3   \n",
              "1  20250715_144536_9170ec05.mp4         4  phone                      31   \n",
              "2  20250715_173316_8cb090ab.mp4         5  phone                      27   \n",
              "3  20250718_145802_46039155.mp4         6  phone                     173   \n",
              "4  20250718_145802_46039155.mp4         8  phone                     133   \n",
              "\n",
              "   active_frames_for_track  idle_frames_for_track  active_ratio_for_track  \n",
              "0                        3                      0                     1.0  \n",
              "1                       31                      0                     1.0  \n",
              "2                        0                     27                     0.0  \n",
              "3                        0                    173                     0.0  \n",
              "4                      133                      0                     1.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7780ba73-9181-49a6-b005-6fd961a47502\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>video</th>\n",
              "      <th>track_id</th>\n",
              "      <th>type</th>\n",
              "      <th>total_frames_for_track</th>\n",
              "      <th>active_frames_for_track</th>\n",
              "      <th>idle_frames_for_track</th>\n",
              "      <th>active_ratio_for_track</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>20250715_142638_e37e7821.mp4</td>\n",
              "      <td>3</td>\n",
              "      <td>phone</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20250715_144536_9170ec05.mp4</td>\n",
              "      <td>4</td>\n",
              "      <td>phone</td>\n",
              "      <td>31</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>20250715_173316_8cb090ab.mp4</td>\n",
              "      <td>5</td>\n",
              "      <td>phone</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>20250718_145802_46039155.mp4</td>\n",
              "      <td>6</td>\n",
              "      <td>phone</td>\n",
              "      <td>173</td>\n",
              "      <td>0</td>\n",
              "      <td>173</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20250718_145802_46039155.mp4</td>\n",
              "      <td>8</td>\n",
              "      <td>phone</td>\n",
              "      <td>133</td>\n",
              "      <td>133</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7780ba73-9181-49a6-b005-6fd961a47502')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7780ba73-9181-49a6-b005-6fd961a47502 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7780ba73-9181-49a6-b005-6fd961a47502');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f70082f4-1bd4-44d9-9f6f-18baef8b8296\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f70082f4-1bd4-44d9-9f6f-18baef8b8296')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f70082f4-1bd4-44d9-9f6f-18baef8b8296 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(\\\"No results to summarize\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"video\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"20250715_144536_9170ec05.mp4\",\n          \"20250718_145802_46039155.mp4\",\n          \"20250715_142638_e37e7821.mp4\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"track_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 3,\n        \"max\": 8,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4,\n          8,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"phone\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total_frames_for_track\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 74,\n        \"min\": 3,\n        \"max\": 173,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          31\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"active_frames_for_track\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57,\n        \"min\": 0,\n        \"max\": 133,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          31\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"idle_frames_for_track\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 75,\n        \"min\": 0,\n        \"max\": 173,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"active_ratio_for_track\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5477225575051662,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saved:\n",
            "- /content/drive/MyDrive/Task Video/op4/run_20250903_170408/summary.csv\n",
            "- /content/drive/MyDrive/Task Video/op4/run_20250903_170408/tracks.csv\n",
            "- Annotated videos in: /content/drive/MyDrive/Task Video/op4/run_20250903_170408\n"
          ]
        }
      ]
    }
  ]
}